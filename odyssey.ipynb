{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70fb4f4-a35c-46a2-8388-ff1fbc466139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Dotlas Odyssey 🏞 [40 points]\n",
    "\n",
    "> Data Engineering Assignment\n",
    "\n",
    "> `v1.0` Updated: Sep 2 2023\n",
    "\n",
    "Greetings Traveller,\n",
    "\n",
    "You have embarked on a mighty Odyssey, a journey filled with challenges, mysteries, and opportunities for growth. Your mission, should you choose to accept it, will involve navigating uncharted territories, solving complex problems, and contributing to a mission that will redefine our future. We believe that with your skills, passion, and determination, you can contribute significantly to this mission.\n",
    "\n",
    "In this notebook, you will find a series of assignments designed to test your skills, stretch your capabilities, and ultimately, prepare you for the journey ahead. Each task is a stepping stone towards your ultimate goal, and completing them will not only bring you closer to becoming a part of the Dotlas family but also to making a meaningful impact on the world.\n",
    "\n",
    "Remember, the journey of a thousand miles begins with a single step. So, gear up, stay positive, and embrace the adventure that awaits you.\n",
    "\n",
    "---\n",
    "<img src=\"https://dotlas-website.s3.eu-west-1.amazonaws.com/images/github/banner.png\" width=\"750px\" alt=\"dotlas\">\n",
    "\n",
    "> You are welcome to make your solution for the Dotlas Odyssey public as open-source and add it to your portfolio of projects, but only after all candidates have completed the assignment, or the position has been filled. Sharing your solution publicly while the assignment process is ongoing will result in disqualification from the selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185e6bb9-9782-4ba4-8f7c-1d69385f7c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tools & Technologies 🪛\n",
    "\n",
    "![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)\n",
    "![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white)\n",
    "![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)\n",
    "![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)\n",
    "![Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C.svg?style=for-the-badge&logo=Apache-Spark&logoColor=white)\n",
    "![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)\n",
    "![Numpy](https://img.shields.io/badge/NumPy-013243.svg?style=for-the-badge&logo=NumPy&logoColor=white)\n",
    "![Plotly](https://img.shields.io/badge/Plotly-%233F4F75.svg?style=for-the-badge&logo=plotly&logoColor=white)\n",
    "![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)\n",
    "\n",
    "- This exercise will be carried out using the [Python](https://www.python.org/) programming language and will rely hevily on the [Pandas](https://pandas.pydata.org/) library for data manipulation.\n",
    "- You are also free to use [Polars](https://www.pola.rs/), [Dask](https://www.dask.org/) or [Spark](https://spark.apache.org/docs/latest/api/python/index.html) if you do not want to use Pandas.\n",
    "- You may use any of [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/) or [Plotly](https://plotly.com/python/) packages for data visualization.\n",
    "- We will be using [Jupyter notebooks](https://jupyter.org/) to run Python code in order to view and interact better with our data and visualizations.\n",
    "- You are free to use [Google Colab](https://colab.research.google.com/) which provides an easy-to-use Jupyter interface.\n",
    "- When not in Colab, it is recommended to run this Jupyter Notebook within an [Anaconda](https://continuum.io/) environment\n",
    "- You can use any other Python packages that you deem fit for this project.\n",
    "\n",
    "> ⚠ **Ensure that your Python version is 3.9 or higher**\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1b/Blue_Python_3.9_Shield_Badge.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1cd966-d7cc-45d9-b6be-a73e14d97f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93a1493-4f91-44eb-8fea-a58dd9862ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 1: Extract 🚰 [5]\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/8rEB2xzZcZDnBegHFS/giphy.gif\" width=\"250px\" alt=\"extract\">\n",
    "\n",
    "You have to extract 4 datasets. Each dataset contains partial information about **restaurants in the San Francisco Bay Area, California, US**. The datasets are as follows:\n",
    "1. **Delta**: A pipe delimited csv file that contains restaurants that offer food-delivery\n",
    "2. **Yankee**: A parquet file that contains crowdsourced restaurant data\n",
    "3. **Oscar**: A raw binary file (pickled) that contains restaurants that take reservations, and other high-end places\n",
    "4. **Tango**: A json file that contains all kinds of restaurants\n",
    "\n",
    "You also have one additional dataset called **Lookup Record** that contains the pre-defined mapping of restaurants between various rows of Delta through Tango. Each row in the Lookup table can be considered as a single restaurant and can be assigned to one or more sources based on whether or not they exist for that restaurant. The Lookup records may have imperfect matches between sources, but you can ignore this quality issue and instead consider it as a source of truth for the amalgamation stage.\n",
    "\n",
    "\n",
    "### License 📜\n",
    "\n",
    "The data in this assignment is provided by Dotlas and is available for non-commercial use, such as research previews, and open-source projects. For any commercial use of the data, you must request access and provide all necessary context to [Dotlas](https://www.dotlas.com). Unauthorized commercial use of the data is prohibited. For more details, refer to the `DATA_LICENSE` file in the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05fd1db4-98d8-4bd6-991e-8a9b5ad7e3db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_url: str = (\n",
    "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_delta_dataset.csv\"\n",
    ")\n",
    "yankee_url: str = (\n",
    "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_yankee_dataset.parquet\"\n",
    ")\n",
    "oscar_url: str = (\n",
    "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_oscar_dataset.pkl\"\n",
    ")\n",
    "tango_url: str = (\n",
    "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_tango_dataset.json\"\n",
    ")\n",
    "lookup_record: str = (\n",
    "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_master_record.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27433a9e-42d7-431e-91ef-f986f85acb4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.1 Read Data 🔀 [2]\n",
    "\n",
    "Load each source data file into separate DataFrames. Do not download the files locally to the file system but instead read them directly in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "614df150-1841-429c-bed5-077b583b9090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff09fae-1323-439a-8c9b-817dac8cbdc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.2 Exploratory Data Analysis 📊 [3]\n",
    "\n",
    "Perform a simple exploratory data analysis of each dataset to understand its structure, null values, nature of the data, data types, duplicate analysis, data quality and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c666aa55-88c5-4535-bc59-db736546ba66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb3df62-e5e1-42ba-92b2-ae527c1d19fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 2: Join and Knit 🧵 [15] \n",
    "\n",
    "<img src=\"https://media.giphy.com/media/rytLWOErAX1F6/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
    "\n",
    "\n",
    "In this section, our goal is to combine four different datasets into one single dataset, which will give us more detailed and comprehensive information about each restaurant. We will use the Lookup records table to join all  Each of the four datasets contains unique information about a restaurant (for example, the Dining Style, the Executive Chef, and the Meals Offered), but they also have some common information (such as the Restaurant Name, Location, Address, Operating Hours, Website, and Telephone Number). \n",
    "\n",
    "We want to merge all this information carefully to create one complete dataset, which we will call 'Table Z'. This process involves handling both the unique and common pieces of information from the four datasets (which we will call 'Tables A, B, C, and D') to ensure that we have all the important details about each restaurant, without any repetition or confusion. \n",
    "\n",
    "Here is a step-by-step example of how we would merge 'Tables A, B, C, and D' to create 'Table Z':\n",
    "\n",
    "**Table A**\n",
    "\n",
    "| id | restaurant_name | area | categories | rating | website | price_range | executive_chef |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | The Good Food | Downtown | Italian,Seafood | 4.5 | goodfood.com | $11-30 | John Doe |\n",
    "| 2 | Taste of Asia | Midtown | Asian,Thai | 4.0 | tasteofasia.com | $10-25 | Jane Doe |\n",
    "\n",
    "**Table B**\n",
    "\n",
    "| id | name | location | type | rating | website | price_class | meals_offered |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | The Good Food | Downtown | Italian | 4.5 | goodfood.com | 2 | Lunch,Dinner |\n",
    "| 3 | Spicy Hut | Uptown | Indian,Spicy | 4.2 | spicyhut.com | 3 | Lunch,Dinner,Brunch |\n",
    "\n",
    "**Table C**\n",
    "\n",
    "| id | restaurant | location | categories | telephone_number | price_range |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 2 | Taste of Asia | Midtown | Asian,Thai | 1234567890 | $10-25 |\n",
    "| 4 | Fresh & Healthy | Suburbs | Vegetarian,Vegan | 9876543210 | $15-30 |\n",
    "\n",
    "**Table D**\n",
    "\n",
    "| id | restaurant_name | area | type | phone_number | website | executive_chef |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 3 | Spicy Hut | Uptown | Indian | 1234567890 | spicyhut.com | Raj Patel |\n",
    "| 4 | Fresh & Healthy | Suburbs | Vegetarian | 9876543210 | freshandhealthy.com | Emily Brown |\n",
    "\n",
    "We want to merge tables A, B, C, and D into a single table Z. However, we need to handle the overlapping and exclusive columns carefully to avoid duplication and to ensure a single source of truth. We will use coalescing to handle overlapping columns with different names (e.g., 'telephone_number' and 'phone_number') and combine values when necessary (e.g., 'categories' and 'type'). We will also choose the most appropriate value for columns that appear in multiple tables but have different values (e.g., 'price_range' and 'price_class').\n",
    "\n",
    "**Table Z**\n",
    "\n",
    "| id | name | location | categories | rating | website | price_range | executive_chef | meals_offered | telephone_number |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | The Good Food | Downtown | Italian,Seafood | 4.5 | goodfood.com | $11-30 | John Doe | Lunch,Dinner | |\n",
    "| 2 | Taste of Asia | Midtown | Asian,Thai | 4.0 | tasteofasia.com | $10-25 | Jane Doe | | 1234567890 |\n",
    "| 3 | Spicy Hut | Uptown | Indian,Spicy | 4.2 | spicyhut.com | $25-40 | Raj Patel | Lunch,Dinner,Brunch | 1234567890 |\n",
    "| 4 | Fresh & Healthy | Suburbs | Vegetarian,Vegan | | freshandhealthy.com | $15-30 | Emily Brown | | 9876543210 |\n",
    "\n",
    "In table Z, we have combined the information from tables A, B, C, and D into a single source of truth. For example, the 'name' column in table Z is derived from the 'restaurant_name' or 'name' columns in the other tables, and the 'telephone_number' column in table Z is derived from the 'telephone_number' or 'phone_number' columns in the other tables. Similarly, the 'categories' column in table Z is a combination of the 'categories' and 'type' columns from the other tables.\n",
    "\n",
    "---\n",
    "\n",
    "### Tips 💡 \n",
    "- Remember to clean up columns in the 4 source datasets where necessary that would make the join more seamless, instead of retrofitting ad-hoc fixes after the fact.\n",
    "- If you have two or more columns with same or similar names or two or more different columns that are representing the same kind of data - then you have not reached the final output\n",
    "- All values in a single column should have the same data type. A type can be simple like `integer`, `float`, `string`, or complex like `list[str]` or `list[int]`. Try to avoid columns that result in having `json` or `dict` values.\n",
    "- The underlying four datasets and lookup records may have duplicates, so watch out for bugs in the join\n",
    "\n",
    "> 📝 It's important to note that while this example is straightforward, you may encounter some ambiguity when working with the actual datasets Delta through Tango. **The main objective is to construct a dataset that provides a comprehensive and detailed context for each restaurant**. There will be instances where you'll need to make subjective decisions, such as prioritizing one dataset over another or creating a new field to harmonize different representations of the same data across datasets (e.g., your own classification of price range). These decisions are up to you, and we encourage you to develop your own approach and rationale for resolving these issues. Just make sure to explain your choices and reasoning in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe0ed02-a238-4884-9ac4-69cac62805c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6311a13-fd83-4ec1-86ea-a241ce1a0419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 3: Web Harvesting 🕸 [15]\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/wSeaJHrOckToQ/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
    "\n",
    "In this section, your task is to enhance the current dataset by navigating to the restaurant websites specified in the data, retrieving any incomplete information, and collecting additional features that could be essential for the analysis. This will involve accessing the URLs of restaurant websites listed in the dataset, and potentially exploring linked pages from the initial page to gather required information. The depth of pages you explore is at your discretion, as long as the added information substantially benefits the final dataset.\n",
    "\n",
    "Tasks include:\n",
    "\n",
    "- **Filling Missing Data**: Identify and fill in missing values in the dataset, such as restaurant descriptions, operating hours, and contact information.\n",
    "- **Extracting New Features**: Extract additional valuable information from the restaurant websites, such as:\n",
    "  - **Social Media Handles**: Links to Facebook, Twitter, Instagram, LinkedIn pages.\n",
    "  - **Awards and Recognitions**: Any awards or recognitions received by the restaurant.\n",
    "  - **Special Events or Offers**: Information on any special events, promotions, or discounts.\n",
    "  - Anything else that you feel may be relevant and valuable\n",
    "\n",
    "Be respectful of the website's `robots.txt` file and terms of service, and ensure your web scraping script is efficient, accurate, and does not overload the servers. You will be graded on the efficiency and accuracy of your script, as well as the completeness and usefulness of the extracted data. Ensure to update the dataset without altering its existing structure and content.\n",
    "\n",
    "> You may need to additionally join and clean up the resultant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f431ae1a-d68b-4f2a-9f47-6615079a9f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "823e734d-70e3-4270-88df-2cf86471dac7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 4: Data Quality Checks ✅ [5]\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/NS7gPxeumewkWDOIxi/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
    "\n",
    "In this section, you are expected to devise and execute a series of data quality tests on the dataframe. Your objective is to identify and address potential edge cases that could affect the downstream use of the dataset. You may use [Great Expectations](https://greatexpectations.io/) or any other tool of your preference to run the tests and profile the data.\n",
    "\n",
    "\n",
    "- **Test Design**: Outline the data quality tests you plan to run. Explain why you have chosen these tests and what potential issues they could uncover.\n",
    "- **Test Execution**: Execute the designed tests on the dataframe. Document the results of each test, including any discrepancies or anomalies identified.\n",
    "- **Data Profiling**: Perform a thorough profiling of the data to understand its characteristics and quality. This may include understanding the distribution of different features, identifying outliers, or assessing the completeness and uniqueness of the data.\n",
    "- **Edge Case Identification**: Discuss any edge cases you have identified during the testing and profiling process. Explain how these edge cases could affect the downstream use of the dataset and propose potential solutions to address them.\n",
    "\n",
    "> 📝 Your ability to design comprehensive tests, identify and address edge cases, and thoroughly profile the data will be crucial in this section. Be sure to document your process and findings clearly and concisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7820a32-6392-4d72-ad17-fcc4f3ac54de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80efd06e-1048-40f1-a01c-3c09f3b5ea73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Save your results in a parquet file and upload them to your private fork of the GitHub Repository."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "odyssey",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
